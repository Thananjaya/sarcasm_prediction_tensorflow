{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarcasm Prediction using Deep Learning\n",
    "\n",
    "This project is meant for differentiating whether the given passage is sarcasm or not. The Data's are captured in the json format and its been loaded afterwards for processing. \n",
    "\n",
    "#### Why Deep Learning?\n",
    "\n",
    "The data to be processed is huge, which results in large memory space and time for processing.Deep Learning out perform other techniques if the data size is large.As per Andrew Ng, the chief scientist of China’s major search engine Baidu and one of the leaders of the Google Brain Project, “The analogy to deep learning is that the rocket engine is the deep learning models and the fuel is the huge amounts of data we can feed to these algorithms.”\n",
    "\n",
    "##### Note:\n",
    "\n",
    "This project is constructed using keras, a high-level neural networks API which is written using Python and capable of running over TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About the dataset\n",
    "\n",
    "The dataset is saved as json file, which is loaded using context manager. Both the independent and dependent feature, headline and is_sarcastic respectively, is saved in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sarcasm.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = []\n",
    "sarcastic = []\n",
    "for content in data:\n",
    "    headline.append(content[\"headline\"])\n",
    "    sarcastic.append(content[\"is_sarcastic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset\n",
    "\n",
    "Dataset is splitted in to training dataset and testing dataset. Model will be fitted using training dataset and validated using new datasets which is not seed by the model during training, so called testing dataset. In this way, we can able to calculate the perfomance of the model.\n",
    "\n",
    "Slicing function, provided by python, is used for this purpose!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = headline[:training_size]\n",
    "testing_data = headline[training_size:]\n",
    "training_label = sarcastic[:training_size]\n",
    "testing_label = sarcastic[training_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keras Tokenizer\n",
    "\n",
    "Machine learning models take vectors (array of integers) as input. When working with text, we should come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. Tokenizer class allows to vectorize a text corpus (collection of written text), by turning each text into either a sequence of integers or into a vector where the coefficient for each token could be binary, based on word count or based on the tf-idf (term frequency-inverse document frequency).\n",
    "\n",
    "Keras Toeknizer takes,\n",
    "* num_words: Maximum number of words to keep, based on the word frequency. Only those words with the maximum frequency will be kept.\n",
    "* oov_token:  used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "\n",
    "Do refer the [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) document for other input options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 10000\n",
    "oov_token = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =Tokenizer(num_words=vocabulary, oov_token = oov_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fits_on_texts\n",
    "\n",
    "Updates internal vocabulary based on a list of texts.\n",
    "\n",
    "#### text_to_sequences\n",
    "\n",
    "Transforms each text in texts to a sequence of integers. Only the most frequent words are taken in to account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tokenizer.texts_to_sequences(training_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = tokenizer.texts_to_sequences(testing_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Sequence Padding\n",
    "\n",
    "This function transforms a lists of integers into a 2D Numpy array.\n",
    "\n",
    "pad_seqeunce takes,\n",
    "* sequences: List of lists, where each element is a sequence.\n",
    "* maxlen: Int, maximum length of all sequences.\n",
    "* padding: String, 'pre' or 'post':\n",
    "    pad either before or after each sequence.\n",
    "* truncating: String, 'pre' or 'post':\n",
    "    remove values from sequences larger than\n",
    "    `maxlen`, either at the beginning or at the end of the sequences.\n",
    "\n",
    "Do refer the [pad_sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) for other input options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =  100\n",
    "trunc ='post'\n",
    "padding ='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_training_data = pad_sequences(training_data, padding=padding, truncating=trunc, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_testing_data = pad_sequences(testing_data, padding=padding, truncating=trunc, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(padded_training_data)\n",
    "testing_data = np.array(padded_testing_data)\n",
    "training_label = np.array(training_label)\n",
    "testing_label = np.array(testing_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Sequential Model\n",
    "\n",
    "The `sequential` model is linear stack of layers. You can create a `Sequential model` by passing a list of layer instances to the constructor.\n",
    "\n",
    "##### Embedding Layer\n",
    "\n",
    "Turns positive integers into dense vectors of fixed size. This layer can only be used as the first layer in a model.\n",
    "\n",
    "Embedding layer takes,\n",
    "* input_dim: List of lists, where each element is a sequence.\n",
    "* output_dim: Int, maximum length of all sequences.\n",
    "* input_length:  Length of input sequences\n",
    "\n",
    "Check [embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) documentation for other details\n",
    "\n",
    "##### GlobalAveragePooling1D\n",
    "\n",
    "The 1D Global average pooling block takes a 2-dimensional tensor of size (input size) x (input channels) and computes the maximum of all the (input size) values for each of the (input channels)\n",
    "\n",
    "#### Activation Layer\n",
    "\n",
    "Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. Without a non-linear function doesn’t matter how many hidden layers we attach in the neutral net all will behave in the same way.Neuron cannot learn with just a linear function attached to it, it requires a non-linear activation function to learn as per the difference w.r.t error.\n",
    "\n",
    "##### Relu Activation\n",
    "\n",
    "The rectified linear activation function is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero\n",
    "\n",
    "##### Sigmoid\n",
    "\n",
    "The sigmoid activation function, also called the logistic function, is traditionally a very popular activation function for neural networks. The input to the function is transformed into a value between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Embedding(vocabulary, embedding_dim, input_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.GlobalAveragePooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(24, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 11709 samples\n",
      "Epoch 1/25\n",
      "15000/15000 - 3s - loss: 0.6863 - accuracy: 0.5531 - val_loss: 0.6786 - val_accuracy: 0.5677\n",
      "Epoch 2/25\n",
      "15000/15000 - 2s - loss: 0.6222 - accuracy: 0.6592 - val_loss: 0.5011 - val_accuracy: 0.8147\n",
      "Epoch 3/25\n",
      "15000/15000 - 2s - loss: 0.3910 - accuracy: 0.8478 - val_loss: 0.3798 - val_accuracy: 0.8376\n",
      "Epoch 4/25\n",
      "15000/15000 - 2s - loss: 0.3031 - accuracy: 0.8821 - val_loss: 0.3584 - val_accuracy: 0.8434\n",
      "Epoch 5/25\n",
      "15000/15000 - 2s - loss: 0.2571 - accuracy: 0.9017 - val_loss: 0.3536 - val_accuracy: 0.8436\n",
      "Epoch 6/25\n",
      "15000/15000 - 2s - loss: 0.2236 - accuracy: 0.9158 - val_loss: 0.3712 - val_accuracy: 0.8370\n",
      "Epoch 7/25\n",
      "15000/15000 - 2s - loss: 0.1965 - accuracy: 0.9249 - val_loss: 0.3489 - val_accuracy: 0.8523\n",
      "Epoch 8/25\n",
      "15000/15000 - 2s - loss: 0.1729 - accuracy: 0.9355 - val_loss: 0.3751 - val_accuracy: 0.8452\n",
      "Epoch 9/25\n",
      "15000/15000 - 2s - loss: 0.1547 - accuracy: 0.9443 - val_loss: 0.3652 - val_accuracy: 0.8511\n",
      "Epoch 10/25\n",
      "15000/15000 - 2s - loss: 0.1374 - accuracy: 0.9531 - val_loss: 0.3863 - val_accuracy: 0.8459\n",
      "Epoch 11/25\n",
      "15000/15000 - 2s - loss: 0.1238 - accuracy: 0.9597 - val_loss: 0.3960 - val_accuracy: 0.8482\n",
      "Epoch 12/25\n",
      "15000/15000 - 2s - loss: 0.1107 - accuracy: 0.9624 - val_loss: 0.4135 - val_accuracy: 0.8460\n",
      "Epoch 13/25\n",
      "15000/15000 - 2s - loss: 0.0997 - accuracy: 0.9671 - val_loss: 0.4414 - val_accuracy: 0.8454\n",
      "Epoch 14/25\n",
      "15000/15000 - 2s - loss: 0.0894 - accuracy: 0.9731 - val_loss: 0.4559 - val_accuracy: 0.8446\n",
      "Epoch 15/25\n",
      "15000/15000 - 2s - loss: 0.0799 - accuracy: 0.9761 - val_loss: 0.4793 - val_accuracy: 0.8411\n",
      "Epoch 16/25\n",
      "15000/15000 - 2s - loss: 0.0736 - accuracy: 0.9779 - val_loss: 0.5260 - val_accuracy: 0.8306\n",
      "Epoch 17/25\n",
      "15000/15000 - 2s - loss: 0.0656 - accuracy: 0.9804 - val_loss: 0.5350 - val_accuracy: 0.8368\n",
      "Epoch 18/25\n",
      "15000/15000 - 2s - loss: 0.0602 - accuracy: 0.9831 - val_loss: 0.5801 - val_accuracy: 0.8335\n",
      "Epoch 19/25\n",
      "15000/15000 - 2s - loss: 0.0543 - accuracy: 0.9849 - val_loss: 0.5948 - val_accuracy: 0.8281\n",
      "Epoch 20/25\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_data, training_label, epochs=25, validation_data=(testing_data, testing_label), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plot(string, history):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_plot('accuracy', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_plot('loss', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\n",
    "    \"I am actually not funny. I am just mean and people think I am joking\",\n",
    "    \"I’d be fine if there were not so much blood in my alcohol system\",\n",
    "    \"If you need so much space, there is always NASA\",\n",
    "    \"game of thrones season finale showing this sunday night\"\n",
    "]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding, truncating=trunc)\n",
    "print(model.predict(padded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
